\section{Accountability}
\subsection{Definition of accountability}
Accountability is just as important as anything else. We tend to combine accountability with
responsibility, and then use the words interchangeably. I'd like to think that this is because
that's exactly how closely related they are. In the case of ethics, the definition of the principle
of accountability is almost identical to Merriam-Webster's definition of responsibility: "able to 
answer for one's conduct and obligations." In terms of AI, this means that the AI should be
able to answer for anything that it creates or says. Accountability is a fairly important principle,
especially easy to see in court and the medical field.

\subsection{How does accountability relate to AI?}
When we take a look at this in those fields, we see examples with a person making assertions to
another person. This person should, in theory, be more knowledgable about the subject than the 
second person receiving the advice. For example, in the medical field, a medical practitioner,
be that a surgeon, doctor, or nurse, should be able to make a diagnosis of the patient. Said
patient should give their symptoms to the doctor, and then receieve their diagnosis. However,
the patient does have the option to appeal to the doctor and say that their diagnosis was
incorrect. This applies almost the same way to the judicial system. The judge will have all
of the information laid out in front of them, and they will create a ruling based off of all
that information. The defendent and/or prosecutor both still have the option to debate this
ruling, to a certain extent. Obviously there's other limitations due to the fact that it's a
legal matter. In both of these examples, the "more knowledgable" person (the judge and doctor)
are making decisions that affect the "less knowledgable" people (defendent and patient),
however the judges and doctors must still be accountable for the rulings or diagnoses that
they've issued. This is where AI fails to meet expectations and requirements for responsibility.
Take another example, this time using an AI driven car - something close to a Tesla, with its
"hands-off driving" modes, but with even less user interaction. Assuming the AI controls this
car totally and completely, we would assume that it is also fully responsible for its own
actions while exerting this control. However, this is not so. If the car is involved in an
accident of any sort, either involving another car or (God forbid) a pedestrian, the AI is in
no way legally responsible for that accident. We should also remember that the accident can
cause the death of a person, which in many court cases results in the survivor, who allegedly
caused the accident, being charged with a crime such as vehicular manslaughter and negligent
driving. In our hypothetical car accident, the person that was in the driver seat at the time
may be the person charged with these crimes, simply because the AI cannot be held responsible
for its own actions.

As I mentioned while discussing transparency, AI cannot create an explanation as to how or why
it reached any given conclusion. It constantly has no understanding of what it's saying or
why it might be saying what it is. Therefore, there's no way that AI can be accountable for 
any of its decisions or results. The true problem with this isn't that it can't be explained
right now, but more so that the AI is still being used in various places. The doctor that was
giving you your diagnosis or the judge sentencing you to 25 years in prison are being influenced
by technology that cannot be fully explained. You would no longer be able to appeal to the doctor
and say that the diagnosis is wrong, because the doctor is just the AI. You would never be able
to ask the judge why they issued their ruling because the AI couldn't explain itself either.

There are some great examples of this happening all over the internet. To name one, there have
been studies done that have proven that AI can be extremely biased. Since the AI must be trained
on some sort of data, it follows that its data could also be biased, driving the AI in a direction
of bias. The problem here is that even when fed what we would consider non-biased data, the AI
may still end up becoming biased towards a specific group of individuals, or, more specifically,
the majority of black people. However, this isn't a discussion about the presence of racism in 
the media or general populace. <insert evidence from some article here>
